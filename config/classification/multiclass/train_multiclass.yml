model:
  _target_: Densenet121 # имя клаccа. Сам класс будет сконструирован в registry по этому имени
  num_classes: &num_classes 2
  mode: Classification
  # path: 'our_models/best.pth' # Путь до расположения вашей локальной модели
  # is_local: False # True если обучаете локально загруженную модель
  # diff_classes_flag: True # Указать True, если есть разница в кол-ве классов
  # old_num_classes: 18 # Если diff_classes_flag=True, то указать кол-во классов в предобученной модели

args: # Различные аргументы для Catalyst
  expdir: src/classification # Путь до нашего эксперимента, с файлом `__init__`, в котором импортируется Runner, и, опционально, регистрируются все дополнительные сущности: model, callback, criterion, etc
  logdir: logs # Путь в который будут сохранятся логи
  verbose: True # Нужно ли выводить на консоль информацию об обучении
  seed: 42 # сид обучения для PyTorch, Numpy, Python и Tensorflow
  deterministic: True # Нужно ли использовать deterministic CuDNN
  benchmark: True # Нужно ли использовать CuDNN benchmark

runner: # Параметры для инициализации Runner
   _target_: MulticlassSupervisedRunner
   input_key: &model_input "features"
   output_key: &model_output "logits"
   target_key: &model_target "targets"
   loss_key: &model_loss "loss"

engine: # Параметры для distributed training и NVIDIA Apex
  _target_: DeviceEngine
  # device: cuda:0

class_names: &class_names ["sigi", "other"]

loggers:
# Встроенные логеры каталиста
#(Возможные логеры: https://github.com/catalyst-team/catalyst/tree/master/catalyst/loggers)
  console:
    _target_: ConsoleLogger
  mlflow:
    _target_: CustomMLflowLogger
    experiment: 'sigi'
    class_names: *class_names

  # tensorboard:
  #   _target_: TensorboardLogger
  #   logdir: "./logs/ui"

stages: # Словарь всех стадий Catalyst, для обучения и/или инфера. Содержат ключевые слова с параметрами, которые применятся ко всем стейджам, так и сами имена стейджей
  stage:
    criterion: # Параметры для лосс-функции
      _target_: CrossEntropyLoss

    optimizer: # Параметры оптимизатора
      _target_: Adam
      lr: 0.0001
    scheduler: # Подключение кастомного шедулера
      _key_value: False

      _target_: CustomScheduler
      delay_epochs: 20
      total_epochs: 50
      eta_min: 0.00001
    callbacks:
      # Подключение колбэков каталиста
      # (Возможные колбэки: https://github.com/catalyst-team/catalyst/tree/master/catalyst/callbacks)

      optimizer: # Параметры для оптимизатора
        _target_: OptimizerCallback
        metric_key: *model_loss
      accuracy:
        _target_: AccuracyCallback
        input_key: *model_output
        target_key: *model_target
      loss:
        _target_: CriterionCallback
        input_key: *model_output
        target_key: *model_target
        metric_key: *model_loss
      verbose:
        _target_: TqdmCallback
      saver: # Сохранение 3-х лучших моделей эксперимента
        _target_: CheckpointCallback
        logdir: "logs/checkpoints"
        save_n_best: 3
      # prunning: # коллбэк для прунинга модели. Смотри документацию в Readme
      #   _target_: PruningCallback
      #   pruning_fn: random_unstructured # функция из модуля torch.nn.utils.prune или ваш на основе BasePruningMethod. Может быть строкой, например. " l1_unstructured ". Подробнее см. В документации по pytorch.
      #   prune_on_epoch_end: True # флаг для прунинга в конце стэйджа
      #   amount: 0.5 # количество параметров для обрезки. Если с плавающей точкой, должно быть от 0,0 до 1,0 и представляют собой часть параметров, подлежащих сокращению. Если int, он представляет собой абсолютное число параметров для обрезки.
      #   l_norm: 2
      quantization:
        _target_: QuantizationCallback
        logdir: "./logs" # Путь для сохранение модели после квантизации
      # Подключение кастомных колбэков
      #(Возможные колбэки: ./src_multilabel/callbacks/)
      onnx_saver: # Конвертация в onnx формат
        _target_: OnnxSaveCallback
        out_dir: "./logs/onnx"
        checkpoint_names: ["best", "best_full"]
      torchscript_saver: # Конвертация в torchscript формат
        _target_: TorchscriptSaveCallback
        out_dir: "./logs/torchsript"
        checkpoint_names: ["last", "best_full"]

      iner:
        _target_: MulticlassCustomAccuracy
        logits: *model_output
        targets: *model_target
        image_name: image_name
        loaders: valid

      save_metrics:
        _target_: SaveMetricInFileCallback
        required_metrics: 
          iner: 
            predicted_images: "logs/csv/subm_file.csv"
      custom_mlflow: # Логирование ошибочных фотографий в mlflow
        _target_: MLFlowMulticlassLoggingCallback
        logging_image_number: 10
      # custom_tensorboard: # Логирование ошибочных фотографий в tensorboard
      #   _target_: TensorboardMulticlassLoggingCallback
      #   logging_image_number: 10
        
    data: # Подключение данных и параметров обучения
      shuffle: false
      train_dir: "./train_dataset_sigi/"
      train_image_dir: "images/train"
      test_image_dir: "images/test"
      train_meta: "train_metadata.csv"
      test_meta: "test_metadata.csv"
      transform_path: "config/augmentations/light.yml" # Режим аугментаций данных (Возможны: light, medium, hard(./config/augmentations/))
    loaders: &loaders
      batch_size: 5 # Размер батча для всех стейджей
      num_workers: 1 #для локальной поставить 0
    num_epochs: 3 # Количество эпох эксперимента
    valid_loader: valid
    main_metric: accuracy01
    minimize_metric: False
