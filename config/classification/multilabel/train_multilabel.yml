model:
  _target_: Densenet121 # имя клаccа. Сам класс будет сконструирован в registry по этому имени
  num_classes: &num_classes 18
  path: 'our_models/best.pth' # Путь до расположения вашей локальной модели
  is_local: False # True если обучаете локально загруженную модель
  diff_classes_flag: True # Указать True, если есть разница в кол-ве классов
  old_num_classes: 2 # Если diff_classes_flag=True, то указать кол-во классов в предобученной модели

args: # Различные аргументы для Catalyst
  expdir: src/classification # Путь до нашего эксперимента, с файлом `__init__`, в котором импортируется Runner, и, опционально, регистрируются все дополнительные сущности: model, callback, criterion, etc
  logdir: logs # Путь в который будут сохранятся логи
  verbose: True # Нужно ли выводить на консоль информацию об обучении
  seed: 42 # сид обучения для PyTorch, Numpy, Python и Tensorflow
  deterministic: True # Нужно ли использовать deterministic CuDNN
  benchmark: True # Нужно ли использовать CuDNN benchmark
  threshold: &threshold 0.5

runner: # Параметры для инициализации Runner
   _target_: MultilabelSupervisedRunner
   input_key: &model_input "features"
   output_key: &model_output "logits"
   target_key: &model_target "targets"
   loss_key: &model_loss "loss"

engine: # Параметры для distributed training и NVIDIA Apex
  _target_: DeviceEngine
  # device: cuda:0
class_names: &class_names ['anti-aerosol', 'cap', 'hood', 'face', 'gas-aerosol', 'helmet', 'screen', 'panoram', 'glasses_open', 'glasses_close','screen_yellow', 'anti_aerosol_error', 'glasses_open_error', 'gas_aerosol_error','glasses_close_error', 'helmet_error', 'screen_error', 'screen_yellow_error']
loggers:
# Встроенные логеры каталиста
#(Возможные логеры: https://github.com/catalyst-team/catalyst/tree/master/catalyst/loggers)
  mlflow:
    _target_: CustomMLflowLogger
    experiment: 'kcm'
    class_names: *class_names
    # tracking_uri: http://mlflow.ddc.itnap.ru:5000/
    # registry_uri: https://storage.yandexcloud.net/

  tensorboard:
    _target_: TensorboardLogger
    logdir: "./logs/ui"

stages: # Словарь всех стадий Catalyst, для обучения и/или инфера. Содержат ключевые слова с параметрами, которые применятся ко всем стейджам, так и сами имена стейджей
  stage:
    criterion: # Параметры для лосс-функции
      _target_: BCEWithLogitsLoss

    optimizer: # Параметры оптимизатора
      _target_: Adam
      lr: 0.001
    scheduler: # Подключение кастомного шедулера
      _key_value: False

      _target_: CustomScheduler
      delay_epochs: 10
      total_epochs: 50
      eta_min: 0.0000001

    callbacks:
    # Подключение колбэков каталиста
    # (Возможные колбэки: https://github.com/catalyst-team/catalyst/tree/master/catalyst/callbacks)

      f1: # f1-score метрика
        _target_: MultilabelPrecisionRecallF1SupportCallback
        input_key: for_metrics
        target_key: *model_target
        num_classes: *num_classes
        zero_division: 0
      optimizer: # Параметры для оптимизатора
        _target_: OptimizerCallback
        metric_key: *model_loss
      accuracy:
        _target_: MultilabelAccuracyCallback
        input_key: *model_output
        target_key: *model_target
        threshold: *threshold
      loss:
        _target_: CriterionCallback
        input_key: *model_output
        target_key: *model_target
        metric_key: *model_loss
      saver: # Сохранение 3-х лучших моделей эксперимента
        _target_: CheckpointCallback
        logdir: "logs/checkpoints"
        save_n_best: 3
      prunning: # коллбэк для прунинга модели
        _target_: PruningCallback
        pruning_fn: l1_unstructured # функция из модуля torch.nn.utils.prune или ваш на основе BasePruningMethod. Может быть строкой, например. " l1_unstructured ". Подробнее см. В документации по pytorch.
        prune_on_stage_end: True # флаг для прунинга в конце стэйджа
        amount: 0.001 # количество параметров для обрезки. Если с плавающей точкой, должно быть от 0,0 до 1,0 и представляют собой часть параметров, подлежащих сокращению. Если int, он представляет собой абсолютное число параметров для обрезки.
      quantization:
        _target_: QuantizationCallback
        logdir: "./logs" # Путь для сохранение модели после квантизации
      # optuna:
      #   _target_: OptunaPruningCallback
      #   minimize: True
      #   loader_key: valid
      #   metric_key: multi_label_accuracy
      #   min_delta: 0.001
      # Подключение кастомных колбэков
      #(Возможные колбэки: ./src_multilabel/callbacks/)
      torchscript_saver: # Конвертация в torchscript формат
        _target_: TorchscriptSaveCallback
        out_dir: "./logs/torchsript"
        checkpoint_names: ["last", "best_full"]
      onnx_saver: # Конвертация в onnx формат
        _target_: OnnxSaveCallback
        out_dir: "./logs/onnx"
        checkpoint_names: ["last", "best_full"]
      infer: #
        _target_: MultilabelInerCallback
        subm_file: "./crossval_log/preds.csv"
      custom_mlflow: # Логирование ошибочных фотографий в mlflow
        _target_: MLFlowMultilabelLoggingCallback
        logging_image_number: 20 # Кол-во фотографий, которые надо логнуть
      custom_tensorboard: # Логирование ошибочных фотографий в tensorboard
        _target_: TensorboardMultilabelLoggingCallback
        logging_image_number: 20 # Кол-во фотографий, которые надо логнуть
        threshold: 0.5

    data: # Подключение данных и параметров обучения
      shuffle: false
      train_dir: "./dataset_classif_0804/"
      train_image_dir: "images"
      test_image_dir: "images"
      train_meta: "train_metadata.csv"
      test_meta: "test_metadata.csv"
      transform_path: "config/classification/augmentations/medium.yml" # Режим аугментаций данных (Возможны: light, medium, hard(./config/classification/augmentations/))
    loaders: &loaders
      batch_size: 1 # Размер батча для всех стейджей
      num_workers: 1 # Для локальной поставить 0
    num_epochs: 50 # Количество эпох эксперимента
    valid_loader: valid
    main_metric: multi_label_accuracy
    minimize_metric: False
